# Human Evaluation UI Implementation Plan

**Project**: AI Agent Quality Testing & Human Evaluation System
**Author**: Claude
**Date**: October 9, 2025
**Version**: 1.0

---

## Table of Contents

1. [Current State Analysis](#current-state-analysis)
2. [Architecture Overview](#architecture-overview)
3. [Phase 1: Database Schema Extension](#phase-1-database-schema-extension)
4. [Phase 2: Token & Cost Tracking System](#phase-2-token--cost-tracking-system)
5. [Phase 3: Web UI Application Architecture](#phase-3-web-ui-application-architecture)
6. [Phase 4: Core UI Features Implementation](#phase-4-core-ui-features-implementation)
7. [Phase 5: Version Capture Strategy](#phase-5-version-capture-strategy)
8. [Phase 6: Integration Points](#phase-6-integration-points)
9. [Phase 7: Deployment & Usage Workflow](#phase-7-deployment--usage-workflow)
10. [Phase 8: Additional Features](#phase-8-additional-features)
11. [Risk Mitigation & Contingencies](#risk-mitigation--contingencies)
12. [Validation Checklist](#validation-checklist)
13. [Estimated Timeline](#estimated-timeline)
14. [Success Criteria](#success-criteria)

---

## Current State Analysis

### What You Already Have

- âœ… Agno agents with SQLite database (sessions, memory, metrics, evals tables)
- âœ… Phoenix integration for tracing at `localhost:6006`
- âœ… AgentOS serving at `localhost:8000` (FastAPI)
- âœ… Token tracking attempt (token_tracker.py - though file doesn't exist yet)
- âš ï¸ Note in code: "not being able to trace total token usage and costs due to agno itself"

### What You Need

Based on your requirements:

1. **Tracing sessions** - View input/output, token consumption per trace
2. **Token & cost calculation** - Automated tracking with manual cost config
3. **Human rating interface** - Manual 1-10 scale ratings
4. **Version management** - Track which agent version produced which output
5. **Analytics dashboard** - Compare quality/cost across versions

---

## Architecture Overview

### System Ports

- **Phoenix**: `localhost:6006` (observability)
- **AgentOS**: `localhost:8000` (agent execution)
- **EvalUI**: `localhost:8888` (NEW - human evaluation interface)

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Writer    â”‚
â”‚  (Human)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 1. Generate story
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EvalUI        â”‚
â”‚  (Port 8888)    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚       â”‚ 2. Triggers agent run
     â”‚       â†“
     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  â”‚   AgentOS   â”‚ â”€â”€â”€â†’ â”‚  Phoenix â”‚
     â”‚  â”‚ (Port 8000) â”‚      â”‚  (6006)  â”‚
     â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚         â”‚ 3. Captures tokens/traces
     â”‚         â†“
     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  â”‚   TokenTracker      â”‚
     â”‚  â”‚   (Middleware)      â”‚
     â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚         â”‚ 4. Stores metrics
     â†“         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      SQLite Database       â”‚
â”‚  - eval_sessions           â”‚
â”‚  - trace_metrics           â”‚
â”‚  - human_ratings           â”‚
â”‚  - agent_versions          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†‘
       â”‚ 5. Human rates output
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Writer    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 1: Database Schema Extension

### New Tables

Add these tables to your existing `database.db`:

```sql
-- Agent version management
CREATE TABLE agent_versions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    version_name VARCHAR(50) UNIQUE NOT NULL,  -- e.g., "v1.0", "v1.1-no-reasoning"
    description TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    config_snapshot TEXT,  -- JSON of agent configuration
    is_active BOOLEAN DEFAULT 1
);

-- Enhanced session tracking with versioning
CREATE TABLE eval_sessions (
    id VARCHAR(255) PRIMARY KEY,
    agent_version_id INTEGER NOT NULL,
    topic TEXT NOT NULL,
    input_text TEXT,
    output_text TEXT,
    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP,
    status VARCHAR(20),  -- 'running', 'completed', 'failed'
    FOREIGN KEY (agent_version_id) REFERENCES agent_versions(id)
);

-- Trace-level token and cost tracking
CREATE TABLE trace_metrics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    eval_session_id VARCHAR(255) NOT NULL,
    agent_name VARCHAR(100) NOT NULL,  -- 'Outline Agent', 'Content Writer', etc.
    trace_id VARCHAR(255),  -- Phoenix trace ID if available
    input_tokens INTEGER DEFAULT 0,
    output_tokens INTEGER DEFAULT 0,
    total_tokens INTEGER DEFAULT 0,
    cost_usd DECIMAL(10, 6) DEFAULT 0,
    latency_ms INTEGER,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (eval_session_id) REFERENCES eval_sessions(id)
);

-- Human ratings
CREATE TABLE human_ratings (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    eval_session_id VARCHAR(255) NOT NULL,
    rater_name VARCHAR(100),  -- Which human rated this
    quality_score INTEGER CHECK(quality_score >= 1 AND quality_score <= 10),
    coherence_score INTEGER CHECK(coherence_score >= 1 AND coherence_score <= 10),
    creativity_score INTEGER CHECK(creativity_score >= 1 AND creativity_score <= 10),
    overall_score INTEGER CHECK(overall_score >= 1 AND overall_score <= 10),
    comments TEXT,
    rated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (eval_session_id) REFERENCES eval_sessions(id)
);

-- Cost configuration for different models
CREATE TABLE model_costs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    model_id VARCHAR(100) UNIQUE NOT NULL,  -- e.g., "claude-sonnet-4-5-20250929"
    input_cost_per_1k DECIMAL(10, 8),  -- Cost per 1K input tokens
    output_cost_per_1k DECIMAL(10, 8),  -- Cost per 1K output tokens
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Pre-populate with Claude Sonnet 4.5 pricing
INSERT INTO model_costs (model_id, input_cost_per_1k, output_cost_per_1k)
VALUES ('claude-sonnet-4-5-20250929', 0.003, 0.015);
```

### Setup Script

Create `setup_eval_ui.py`:

```python
import sqlite3

def setup_database():
    """Create new tables for eval UI"""
    conn = sqlite3.connect('database.db')
    cursor = conn.cursor()

    # Enable WAL mode for better concurrency
    cursor.execute("PRAGMA journal_mode=WAL;")

    # Execute all CREATE TABLE statements
    # ... (paste SQL from above)

    conn.commit()
    conn.close()
    print("âœ… Database schema updated")

if __name__ == "__main__":
    setup_database()
```

### Rationale

- **Separation of concerns**: Versioning, metrics, and ratings are independent
- **Multiple raters**: Allows team of writers to rate independently
- **Flexible cost config**: Can add different models without code changes
- **Links to existing system**: Integrates with your current session management

---

## Phase 2: Token & Cost Tracking System

### Challenge

You noted: "not being able to trace total token usage and costs due to agno itself"

### Solution: Multi-layered Approach

#### 2.1 Create `token_tracker.py`

```python
# token_tracker.py
import sqlite3
from typing import Optional, Dict, Tuple
from datetime import datetime
import json

class TokenTracker:
    """Track tokens and costs for AI agent runs"""

    def __init__(self, db_path: str = "database.db"):
        self.db_path = db_path
        self._init_cost_config()

    def _init_cost_config(self):
        """Load model costs from database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT model_id, input_cost_per_1k, output_cost_per_1k FROM model_costs")
        self.costs = {row[0]: {'input': row[1], 'output': row[2]} for row in cursor.fetchall()}
        conn.close()

    def track_usage(self, eval_session_id: str, agent_name: str,
                    input_tokens: int, output_tokens: int,
                    model_id: str, trace_id: Optional[str] = None,
                    latency_ms: Optional[int] = None) -> float:
        """Record token usage and calculate cost"""
        total_tokens = input_tokens + output_tokens

        # Calculate cost
        cost = 0
        if model_id in self.costs:
            cost = (input_tokens / 1000 * self.costs[model_id]['input'] +
                    output_tokens / 1000 * self.costs[model_id]['output'])

        # Insert into database
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO trace_metrics
            (eval_session_id, agent_name, trace_id, input_tokens, output_tokens,
             total_tokens, cost_usd, latency_ms)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (eval_session_id, agent_name, trace_id, input_tokens,
              output_tokens, total_tokens, cost, latency_ms))
        conn.commit()
        conn.close()

        return cost

    def extract_usage_from_response(self, response) -> Tuple[int, int]:
        """Extract token counts from Anthropic API response"""
        # Anthropic responses have usage attribute
        if hasattr(response, 'usage'):
            return response.usage.input_tokens, response.usage.output_tokens

        # Fallback: try to extract from dict
        if isinstance(response, dict) and 'usage' in response:
            usage = response['usage']
            return usage.get('input_tokens', 0), usage.get('output_tokens', 0)

        return 0, 0

    def get_session_summary(self, eval_session_id: str) -> Dict:
        """Get aggregate metrics for a session"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            SELECT
                SUM(input_tokens) as total_input,
                SUM(output_tokens) as total_output,
                SUM(total_tokens) as total_tokens,
                SUM(cost_usd) as total_cost,
                AVG(latency_ms) as avg_latency,
                COUNT(*) as trace_count
            FROM trace_metrics
            WHERE eval_session_id = ?
        """, (eval_session_id,))
        result = cursor.fetchone()
        conn.close()

        return {
            'total_input_tokens': result[0] or 0,
            'total_output_tokens': result[1] or 0,
            'total_tokens': result[2] or 0,
            'total_cost': result[3] or 0,
            'avg_latency_ms': result[4] or 0,
            'trace_count': result[5] or 0
        }
```

#### 2.2 Token Extraction Strategies

**Strategy 1: Direct from Agno Response**

```python
# If Agno exposes response objects
response = agent.run(message)

# Anthropic SDK returns usage in response.usage
if hasattr(response, 'usage'):
    input_tokens = response.usage.input_tokens
    output_tokens = response.usage.output_tokens
```

**Strategy 2: Monkey-patch Anthropic Client**

```python
# Wrap the Anthropic client to intercept calls
from anthropic import Anthropic

class TrackedAnthropicClient(Anthropic):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.last_usage = None

    def messages_create(self, *args, **kwargs):
        response = super().messages.create(*args, **kwargs)
        self.last_usage = {
            'input_tokens': response.usage.input_tokens,
            'output_tokens': response.usage.output_tokens
        }
        return response
```

**Strategy 3: Query Phoenix OTEL Traces (Fallback)**

```python
# eval_ui/phoenix_client.py
import requests

def fetch_token_usage_from_phoenix(session_id: str) -> list:
    """Query Phoenix GraphQL API for trace data"""
    query = """
    query {
      traces(filter: {tags: {key: "session_id", value: "%s"}}) {
        edges {
          node {
            traceId
            spans {
              name
              attributes {
                key
                value
              }
            }
          }
        }
      }
    }
    """ % session_id

    try:
        response = requests.post(
            'http://localhost:6006/graphql',
            json={'query': query},
            timeout=5
        )
        data = response.json()

        # Parse spans for token usage
        traces = []
        for edge in data.get('data', {}).get('traces', {}).get('edges', []):
            span_data = edge['node']
            # Extract token counts from attributes
            # Phoenix stores these in OpenTelemetry semantic conventions
            traces.append(parse_span_attributes(span_data))

        return traces
    except Exception as e:
        print(f"Failed to fetch from Phoenix: {e}")
        return []

def parse_span_attributes(span_data):
    """Extract token usage from span attributes"""
    attributes = {attr['key']: attr['value'] for attr in span_data.get('attributes', [])}

    return {
        'trace_id': span_data['traceId'],
        'agent_name': attributes.get('agent.name', 'unknown'),
        'input_tokens': int(attributes.get('llm.input_tokens', 0)),
        'output_tokens': int(attributes.get('llm.output_tokens', 0)),
    }
```

---

## Phase 3: Web UI Application Architecture

### Technology Decision: Streamlit

**Why Streamlit?**

- âœ… Pure Python (no JavaScript needed)
- âœ… Built-in components for forms, tables, charts
- âœ… Perfect for internal dashboards
- âœ… Can run on `localhost:8888`
- âœ… Rapid development and iteration
- âœ… Easy for non-developers to use

**Alternative**: React + FastAPI (more complex, 3x longer to build)

### File Structure

```
no-bullshit-agno/
â”œâ”€â”€ agents.py                    # Existing agent code
â”œâ”€â”€ database.db                  # Extended with new tables
â”œâ”€â”€ token_tracker.py             # NEW: Token tracking utility
â”œâ”€â”€ setup_eval_ui.py             # NEW: Database setup script
â”œâ”€â”€ requirements-eval.txt        # NEW: Additional dependencies
â”œâ”€â”€ eval_ui/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                   # Main Streamlit app (entry point)
â”‚   â”œâ”€â”€ database.py              # Database access layer
â”‚   â”œâ”€â”€ versioning.py            # Version management utilities
â”‚   â”œâ”€â”€ phoenix_client.py        # Phoenix API integration
â”‚   â”œâ”€â”€ agent_runner.py          # Wrapper to run agents with tracking
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ 1_ðŸ _Dashboard.py   # Overview & analytics
â”‚   â”‚   â”œâ”€â”€ 2_ðŸ§ª_Run_Test.py    # Execute agent tests
â”‚   â”‚   â”œâ”€â”€ 3_ðŸ“Š_Review_Results.py  # Rate outputs
â”‚   â”‚   â”œâ”€â”€ 4_ðŸ“ˆ_Analytics.py   # Version comparison
â”‚   â”‚   â””â”€â”€ 5_âš™ï¸_Settings.py    # Manage versions & costs
â”‚   â””â”€â”€ components/
â”‚       â”œâ”€â”€ rating_form.py       # Reusable rating component
â”‚       â”œâ”€â”€ trace_viewer.py      # Token/trace display
â”‚       â”œâ”€â”€ charts.py            # Plotting utilities
â”‚       â””â”€â”€ session_card.py      # Session display component
```

### Dependencies

Add to `requirements-eval.txt`:

```
streamlit>=1.28.0
plotly>=5.17.0
pandas>=2.3.3
```

---

## Phase 4: Core UI Features Implementation

### Feature 1: Tracing Sessions (Like Phoenix)

**File**: `eval_ui/components/trace_viewer.py`

```python
import streamlit as st
import pandas as pd
from eval_ui.database import fetch_session, fetch_traces

def display_trace(eval_session_id: str):
    """Display comprehensive trace view"""

    # Fetch session data
    session_data = fetch_session(eval_session_id)
    traces = fetch_traces(eval_session_id)

    # Session header
    st.subheader(f"Session: {eval_session_id}")

    col1, col2, col3 = st.columns(3)
    with col1:
        st.write(f"**Topic**: {session_data['topic']}")
    with col2:
        st.write(f"**Version**: {session_data['version_name']}")
    with col3:
        st.write(f"**Status**: {session_data['status']}")

    # Token metrics summary
    total_input = sum(t['input_tokens'] for t in traces)
    total_output = sum(t['output_tokens'] for t in traces)
    total_cost = sum(t['cost_usd'] for t in traces)

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Input Tokens", f"{total_input:,}")
    col2.metric("Output Tokens", f"{total_output:,}")
    col3.metric("Total Tokens", f"{total_input + total_output:,}")
    col4.metric("Cost", f"${total_cost:.4f}")

    # Agent breakdown table
    st.subheader("Agent Breakdown")
    if traces:
        df = pd.DataFrame(traces)
        st.dataframe(
            df[['agent_name', 'input_tokens', 'output_tokens',
                'total_tokens', 'cost_usd', 'latency_ms']],
            use_container_width=True
        )
    else:
        st.info("No trace data available")

    # Input/Output viewer
    st.subheader("Input/Output")

    with st.expander("View Input", expanded=False):
        st.text_area("Input", session_data.get('input_text', 'N/A'),
                     height=100, disabled=True, key=f"input_{eval_session_id}")

    with st.expander("View Output", expanded=True):
        st.text_area("Output", session_data.get('output_text', 'N/A'),
                     height=300, disabled=True, key=f"output_{eval_session_id}")
```

### Feature 2: Automated Token & Cost Calculation

Already implemented in `token_tracker.py` (Phase 2)

Key points:
- Costs auto-calculated from token counts + `model_costs` table
- Per-trace granularity allows detailed breakdowns
- Supports multiple model configurations

### Feature 3: Human Rating Interface

**File**: `eval_ui/components/rating_form.py`

```python
import streamlit as st
from eval_ui.database import save_rating, get_existing_rating

def rating_form(eval_session_id: str, rater_name: str = None):
    """Interactive rating form for humans"""

    st.subheader("Rate This Story")

    # Check if already rated
    existing = get_existing_rating(eval_session_id, rater_name)

    if existing:
        st.info(f"You already rated this on {existing['rated_at']}")
        if not st.checkbox("Edit rating"):
            return

    with st.form(key=f'rating_form_{eval_session_id}'):
        rater = st.text_input("Your Name", value=rater_name or "", key='rater_name')

        st.write("**Rate on scale 1-10** (1=Poor, 10=Excellent)")

        col1, col2 = st.columns(2)
        with col1:
            quality = st.slider("Quality", 1, 10,
                              existing['quality_score'] if existing else 5,
                              key='quality',
                              help="Overall writing quality, grammar, style")
            coherence = st.slider("Coherence", 1, 10,
                                existing['coherence_score'] if existing else 5,
                                key='coherence',
                                help="Story flow, logical consistency")

        with col2:
            creativity = st.slider("Creativity", 1, 10,
                                 existing['creativity_score'] if existing else 5,
                                 key='creativity',
                                 help="Originality, interesting ideas")
            overall = st.slider("Overall", 1, 10,
                              existing['overall_score'] if existing else 5,
                              key='overall',
                              help="Overall impression")

        comments = st.text_area("Comments (optional)",
                               value=existing['comments'] if existing else "",
                               key='comments',
                               placeholder="What worked well? What could be improved?")

        submitted = st.form_submit_button("Submit Rating" if not existing else "Update Rating")

        if submitted:
            if not rater:
                st.error("Please enter your name")
                return

            save_rating(
                eval_session_id=eval_session_id,
                rater_name=rater,
                quality_score=quality,
                coherence_score=coherence,
                creativity_score=creativity,
                overall_score=overall,
                comments=comments
            )
            st.success("âœ… Rating saved!")
            st.rerun()
```

### Feature 4: Version Management & Analytics

**File**: `eval_ui/pages/4_ðŸ“ˆ_Analytics.py`

```python
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
from eval_ui.database import fetch_all_versions, fetch_version_metrics

def analytics_dashboard():
    st.title("ðŸ“ˆ Version Analytics Dashboard")

    # Version selector
    versions = fetch_all_versions()

    if not versions:
        st.warning("No versions found. Create a version in Settings first.")
        return

    version_names = [v['version_name'] for v in versions]
    selected_versions = st.multiselect(
        "Select Versions to Compare",
        version_names,
        default=version_names[-2:] if len(version_names) >= 2 else version_names
    )

    if not selected_versions:
        st.info("Select at least one version to view analytics")
        return

    # Fetch aggregated metrics
    data = fetch_version_metrics(selected_versions)

    if data.empty:
        st.info("No data available for selected versions")
        return

    # Summary metrics
    st.subheader("Summary")
    cols = st.columns(len(selected_versions))
    for i, version in enumerate(selected_versions):
        version_data = data[data['version_name'] == version]
        with cols[i]:
            st.metric(
                label=version,
                value=f"{version_data['avg_overall_score'].mean():.2f}",
                delta=f"${version_data['avg_cost_usd'].mean():.4f} avg cost"
            )

    # Quality trend over time
    st.subheader("Quality Trend Over Time")
    fig1 = px.line(
        data,
        x='created_at',
        y='avg_overall_score',
        color='version_name',
        title='Average Overall Score by Version',
        markers=True
    )
    fig1.update_layout(
        xaxis_title="Date",
        yaxis_title="Average Overall Score",
        yaxis_range=[0, 10]
    )
    st.plotly_chart(fig1, use_container_width=True)

    # Cost comparison
    st.subheader("Cost Analysis")
    cost_data = data.groupby('version_name').agg({
        'avg_cost_usd': 'mean',
        'avg_total_tokens': 'mean'
    }).reset_index()

    fig2 = go.Figure()
    fig2.add_trace(go.Bar(
        x=cost_data['version_name'],
        y=cost_data['avg_cost_usd'],
        name='Avg Cost',
        text=cost_data['avg_cost_usd'].apply(lambda x: f'${x:.4f}'),
        textposition='auto'
    ))
    fig2.update_layout(
        title='Average Cost per Session by Version',
        xaxis_title='Version',
        yaxis_title='Cost (USD)'
    )
    st.plotly_chart(fig2, use_container_width=True)

    # Token usage comparison
    st.subheader("Token Usage")
    fig3 = px.bar(
        cost_data,
        x='version_name',
        y='avg_total_tokens',
        title='Average Total Tokens by Version',
        text='avg_total_tokens'
    )
    fig3.update_traces(texttemplate='%{text:,.0f}', textposition='outside')
    st.plotly_chart(fig3, use_container_width=True)

    # Detailed metrics table
    st.subheader("Detailed Metrics")
    summary = data.groupby('version_name').agg({
        'avg_overall_score': ['mean', 'std'],
        'avg_quality_score': 'mean',
        'avg_coherence_score': 'mean',
        'avg_creativity_score': 'mean',
        'avg_cost_usd': ['mean', 'std'],
        'avg_total_tokens': 'mean',
        'session_count': 'sum'
    }).round(4)

    st.dataframe(summary, use_container_width=True)

    # Export data
    if st.button("ðŸ“¥ Export Data to CSV"):
        csv = data.to_csv(index=False)
        st.download_button(
            label="Download CSV",
            data=csv,
            file_name=f'version_analytics_{pd.Timestamp.now().strftime("%Y%m%d")}.csv',
            mime='text/csv'
        )
```

---

## Phase 5: Version Capture Strategy

### Challenge

How to automatically track which agent version produced which output?

### Solution

#### 5.1 Version Creation

**File**: `eval_ui/versioning.py`

```python
import sqlite3
import json
from datetime import datetime
from typing import Dict, Optional

def create_version(name: str, description: str, config: Dict) -> int:
    """Create new agent version snapshot"""

    conn = sqlite3.connect('database.db')
    cursor = conn.cursor()

    # Deactivate previous versions
    cursor.execute("UPDATE agent_versions SET is_active = 0")

    # Create new version
    cursor.execute("""
        INSERT INTO agent_versions (version_name, description, config_snapshot, is_active)
        VALUES (?, ?, ?, 1)
    """, (name, description, json.dumps(config)))

    version_id = cursor.lastrowid

    conn.commit()
    conn.close()

    return version_id

def get_active_version() -> tuple:
    """Get currently active version"""
    conn = sqlite3.connect('database.db')
    cursor = conn.cursor()
    cursor.execute("""
        SELECT id, version_name
        FROM agent_versions
        WHERE is_active = 1
        ORDER BY created_at DESC
        LIMIT 1
    """)
    result = cursor.fetchone()
    conn.close()

    if result:
        return result[0], result[1]
    else:
        # Create default version if none exists
        version_id = create_version(
            name="v1.0",
            description="Initial version",
            config={}
        )
        return version_id, "v1.0"

def capture_current_config() -> Dict:
    """Capture current agent configuration from agents.py"""
    # This will read the current agent configurations
    # You can customize this to extract relevant parameters

    from agents import content_team

    # Extract configuration details
    config = {
        'model_id': 'claude-sonnet-4-5-20250929',
        'agents': {
            'outline_agent': {
                'role': 'Create a short story outline based on a given topic',
                'has_tools': True,
                'reasoning': False
            },
            'content_writer': {
                'role': 'Write story based on a given outline',
                'has_tools': False,
                'reasoning': False
            }
        },
        'timestamp': datetime.now().isoformat()
    }

    return config

def list_versions() -> list:
    """Get all versions"""
    conn = sqlite3.connect('database.db')
    cursor = conn.cursor()
    cursor.execute("""
        SELECT id, version_name, description, created_at, is_active
        FROM agent_versions
        ORDER BY created_at DESC
    """)
    versions = [
        {
            'id': row[0],
            'version_name': row[1],
            'description': row[2],
            'created_at': row[3],
            'is_active': bool(row[4])
        }
        for row in cursor.fetchall()
    ]
    conn.close()
    return versions
```

#### 5.2 Auto-Capture During Runs

**File**: `eval_ui/agent_runner.py`

```python
import time
import uuid
from datetime import datetime
import sqlite3
from typing import Optional
from agents import content_team
from token_tracker import TokenTracker
from eval_ui.versioning import get_active_version

class AgentRunner:
    """Wrapper to run agents with automatic tracking"""

    def __init__(self):
        self.tracker = TokenTracker()

    def run_with_tracking(self, topic: str) -> dict:
        """Run agent and capture all metrics"""

        # Get active version
        version_id, version_name = get_active_version()

        # Create eval session
        eval_session_id = f"eval-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{uuid.uuid4().hex[:8]}"

        conn = sqlite3.connect('database.db')
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO eval_sessions (id, agent_version_id, topic, input_text, status)
            VALUES (?, ?, ?, ?, 'running')
        """, (eval_session_id, version_id, topic, topic))
        conn.commit()
        conn.close()

        # Run agents
        try:
            start_time = time.time()
            os_instance = content_team()

            # Run the agent team
            result = os_instance.run(message=topic, session_id=eval_session_id)

            end_time = time.time()
            latency_ms = int((end_time - start_time) * 1000)

            # Extract output text
            output_text = str(result.content) if hasattr(result, 'content') else str(result)

            # Update session with completion
            conn = sqlite3.connect('database.db')
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE eval_sessions
                SET output_text = ?, completed_at = ?, status = 'completed'
                WHERE id = ?
            """, (output_text, datetime.now(), eval_session_id))
            conn.commit()
            conn.close()

            # TODO: Extract token usage
            # This depends on how Agno exposes response objects
            # For now, we'll try to get from response or Phoenix

            return {
                'success': True,
                'eval_session_id': eval_session_id,
                'version_name': version_name,
                'output': output_text,
                'latency_ms': latency_ms
            }

        except Exception as e:
            # Mark as failed
            conn = sqlite3.connect('database.db')
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE eval_sessions
                SET status = 'failed', completed_at = ?
                WHERE id = ?
            """, (datetime.now(), eval_session_id))
            conn.commit()
            conn.close()

            return {
                'success': False,
                'eval_session_id': eval_session_id,
                'error': str(e)
            }
```

---

## Phase 6: Integration Points

### 6.1 Minimal Changes to `agents.py`

Add these imports at the top:

```python
# Add after existing imports
from token_tracker import TokenTracker
from eval_ui.versioning import get_active_version
import time
import uuid
from datetime import datetime
```

### 6.2 Database Access Layer

**File**: `eval_ui/database.py`

```python
import sqlite3
from typing import List, Dict, Optional
import pandas as pd
from datetime import datetime

DB_PATH = 'database.db'

def get_connection():
    """Get database connection"""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn

# Sessions
def fetch_session(eval_session_id: str) -> Dict:
    """Fetch single session details"""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("""
        SELECT es.*, av.version_name
        FROM eval_sessions es
        JOIN agent_versions av ON es.agent_version_id = av.id
        WHERE es.id = ?
    """, (eval_session_id,))
    row = cursor.fetchone()
    conn.close()
    return dict(row) if row else {}

def fetch_all_sessions(limit: int = 100) -> List[Dict]:
    """Fetch recent sessions"""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("""
        SELECT es.id, es.topic, es.status, es.started_at, av.version_name
        FROM eval_sessions es
        JOIN agent_versions av ON es.agent_version_id = av.id
        ORDER BY es.started_at DESC
        LIMIT ?
    """, (limit,))
    rows = cursor.fetchall()
    conn.close()
    return [dict(row) for row in rows]

# Traces
def fetch_traces(eval_session_id: str) -> List[Dict]:
    """Fetch traces for a session"""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("""
        SELECT * FROM trace_metrics
        WHERE eval_session_id = ?
        ORDER BY timestamp
    """, (eval_session_id,))
    rows = cursor.fetchall()
    conn.close()
    return [dict(row) for row in rows]

# Ratings
def save_rating(eval_session_id: str, rater_name: str,
                quality_score: int, coherence_score: int,
                creativity_score: int, overall_score: int,
                comments: str):
    """Save human rating"""
    conn = get_connection()
    cursor = conn.cursor()

    # Check if rating exists
    cursor.execute("""
        SELECT id FROM human_ratings
        WHERE eval_session_id = ? AND rater_name = ?
    """, (eval_session_id, rater_name))

    existing = cursor.fetchone()

    if existing:
        # Update
        cursor.execute("""
            UPDATE human_ratings
            SET quality_score = ?, coherence_score = ?,
                creativity_score = ?, overall_score = ?,
                comments = ?, rated_at = ?
            WHERE id = ?
        """, (quality_score, coherence_score, creativity_score,
              overall_score, comments, datetime.now(), existing[0]))
    else:
        # Insert
        cursor.execute("""
            INSERT INTO human_ratings
            (eval_session_id, rater_name, quality_score, coherence_score,
             creativity_score, overall_score, comments)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (eval_session_id, rater_name, quality_score, coherence_score,
              creativity_score, overall_score, comments))

    conn.commit()
    conn.close()

def get_existing_rating(eval_session_id: str, rater_name: str) -> Optional[Dict]:
    """Check if rating exists"""
    if not rater_name:
        return None

    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("""
        SELECT * FROM human_ratings
        WHERE eval_session_id = ? AND rater_name = ?
    """, (eval_session_id, rater_name))
    row = cursor.fetchone()
    conn.close()
    return dict(row) if row else None

# Versions
def fetch_all_versions() -> List[Dict]:
    """Get all versions"""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("""
        SELECT * FROM agent_versions
        ORDER BY created_at DESC
    """)
    rows = cursor.fetchall()
    conn.close()
    return [dict(row) for row in rows]

def fetch_version_metrics(version_names: List[str]) -> pd.DataFrame:
    """Get aggregated metrics for versions"""
    conn = get_connection()

    placeholders = ','.join('?' * len(version_names))
    query = f"""
        SELECT
            av.version_name,
            es.started_at as created_at,
            AVG(hr.overall_score) as avg_overall_score,
            AVG(hr.quality_score) as avg_quality_score,
            AVG(hr.coherence_score) as avg_coherence_score,
            AVG(hr.creativity_score) as avg_creativity_score,
            AVG(tm_summary.total_cost) as avg_cost_usd,
            AVG(tm_summary.total_tokens) as avg_total_tokens,
            COUNT(DISTINCT es.id) as session_count
        FROM eval_sessions es
        JOIN agent_versions av ON es.agent_version_id = av.id
        LEFT JOIN human_ratings hr ON es.id = hr.eval_session_id
        LEFT JOIN (
            SELECT eval_session_id,
                   SUM(cost_usd) as total_cost,
                   SUM(total_tokens) as total_tokens
            FROM trace_metrics
            GROUP BY eval_session_id
        ) tm_summary ON es.id = tm_summary.eval_session_id
        WHERE av.version_name IN ({placeholders})
        GROUP BY av.version_name, es.id
    """

    df = pd.read_sql_query(query, conn, params=version_names)
    conn.close()
    return df

# Model costs
def update_model_cost(model_id: str, input_cost: float, output_cost: float):
    """Update or insert model cost"""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("""
        INSERT INTO model_costs (model_id, input_cost_per_1k, output_cost_per_1k, updated_at)
        VALUES (?, ?, ?, ?)
        ON CONFLICT(model_id)
        DO UPDATE SET
            input_cost_per_1k = ?,
            output_cost_per_1k = ?,
            updated_at = ?
    """, (model_id, input_cost, output_cost, datetime.now(),
          input_cost, output_cost, datetime.now()))
    conn.commit()
    conn.close()

def get_all_model_costs() -> List[Dict]:
    """Get all model costs"""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM model_costs ORDER BY model_id")
    rows = cursor.fetchall()
    conn.close()
    return [dict(row) for row in rows]
```

### 6.3 Phoenix Integration (Fallback)

**File**: `eval_ui/phoenix_client.py`

```python
import requests
from typing import List, Dict, Optional

PHOENIX_URL = "http://localhost:6006"

def is_phoenix_available() -> bool:
    """Check if Phoenix is running"""
    try:
        response = requests.get(f"{PHOENIX_URL}/healthz", timeout=2)
        return response.status_code == 200
    except:
        return False

def fetch_traces_from_phoenix(session_id: str) -> List[Dict]:
    """Query Phoenix GraphQL API for trace data"""

    if not is_phoenix_available():
        return []

    query = """
    query {
      traces(filter: {tags: {key: "session_id", value: "%s"}}) {
        edges {
          node {
            traceId
            spans {
              name
              attributes {
                key
                value
              }
            }
          }
        }
      }
    }
    """ % session_id

    try:
        response = requests.post(
            f'{PHOENIX_URL}/graphql',
            json={'query': query},
            timeout=10
        )
        data = response.json()

        # Parse spans for token usage
        traces = []
        for edge in data.get('data', {}).get('traces', {}).get('edges', []):
            span_data = edge['node']
            parsed = parse_span_attributes(span_data)
            if parsed:
                traces.append(parsed)

        return traces
    except Exception as e:
        print(f"Failed to fetch from Phoenix: {e}")
        return []

def parse_span_attributes(span_data: Dict) -> Optional[Dict]:
    """Extract token usage from span attributes"""
    attributes = {}
    for attr in span_data.get('attributes', []):
        attributes[attr['key']] = attr['value']

    # Look for token usage in attributes
    # OpenTelemetry semantic conventions for LLMs
    if 'llm.input_tokens' in attributes or 'llm.output_tokens' in attributes:
        return {
            'trace_id': span_data['traceId'],
            'agent_name': attributes.get('agent.name', 'unknown'),
            'input_tokens': int(attributes.get('llm.input_tokens', 0)),
            'output_tokens': int(attributes.get('llm.output_tokens', 0)),
        }

    return None
```

---

## Phase 7: Deployment & Usage Workflow

### 7.1 Setup Steps

1. **Install dependencies**:
```bash
pip install streamlit plotly
```

2. **Run setup script**:
```bash
python setup_eval_ui.py
```

3. **Create initial version**:
```python
from eval_ui.versioning import create_version, capture_current_config

config = capture_current_config()
create_version("v1.0", "Initial baseline version", config)
```

### 7.2 Running the System

**Terminal 1: Phoenix (Optional)**
```bash
python -m phoenix.server.main serve
# Runs on http://localhost:6006
```

**Terminal 2: Agent System**
```bash
python agents.py
# Runs on http://localhost:8000
```

**Terminal 3: Evaluation UI**
```bash
streamlit run eval_ui/app.py --server.port 8888
# Runs on http://localhost:8888
```

### 7.3 Daily Workflow for Writers

1. Open browser to `http://localhost:8888`
2. Navigate to "ðŸ§ª Run Test" page
3. Enter story topic (e.g., "A detective solving a mystery in ancient Rome")
4. Click "Generate Story"
5. Wait for generation (15-30 seconds)
6. Review the output
7. View token breakdown and costs
8. Submit rating using the form
9. Repeat for multiple tests

### 7.4 Workflow for Developers

**When you modify agents**:

1. Make changes to `agents.py` (modify instructions, add tools, etc.)
2. Open Evaluation UI â†’ "âš™ï¸ Settings"
3. Click "Create New Version"
4. Enter version name (e.g., "v1.1-with-reasoning")
5. Add description (e.g., "Enabled reasoning mode for both agents")
6. System captures config snapshot
7. All new test runs automatically tagged with this version

**To compare versions**:

1. Open "ðŸ“ˆ Analytics" page
2. Select versions to compare (e.g., v1.0 vs v1.1)
3. View:
   - Average quality scores over time
   - Cost comparison (did changes increase cost?)
   - Token usage patterns
   - Detailed metrics table

---

## Phase 8: Additional Features

### 8.1 Batch Testing

**File**: `eval_ui/pages/6_âš¡_Batch_Test.py`

```python
import streamlit as st
import pandas as pd
from eval_ui.agent_runner import AgentRunner
import time

def batch_test_page():
    st.title("âš¡ Batch Test Runner")

    st.info("Upload a CSV file with a 'topic' column to run multiple tests in sequence.")

    # File uploader
    uploaded_file = st.file_uploader("Upload Topics CSV", type=['csv'])

    if uploaded_file:
        topics_df = pd.read_csv(uploaded_file)

        if 'topic' not in topics_df.columns:
            st.error("CSV must have a 'topic' column")
            return

        st.write(f"Found {len(topics_df)} topics:")
        st.dataframe(topics_df.head(10))

        if st.button("â–¶ï¸ Run Batch Test", type="primary"):
            runner = AgentRunner()

            progress_bar = st.progress(0)
            status_text = st.empty()
            results = []

            for i, row in topics_df.iterrows():
                topic = row['topic']
                status_text.text(f"Running {i+1}/{len(topics_df)}: {topic[:50]}...")

                result = runner.run_with_tracking(topic)
                results.append({
                    'topic': topic,
                    'eval_session_id': result.get('eval_session_id'),
                    'success': result.get('success'),
                    'version': result.get('version_name')
                })

                progress_bar.progress((i + 1) / len(topics_df))

                # Brief pause between runs
                time.sleep(1)

            status_text.text("âœ… Batch complete!")

            # Show results
            results_df = pd.DataFrame(results)
            st.subheader("Batch Results")
            st.dataframe(results_df)

            # Download results
            csv = results_df.to_csv(index=False)
            st.download_button(
                label="ðŸ“¥ Download Results",
                data=csv,
                file_name=f'batch_results_{pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")}.csv',
                mime='text/csv'
            )

if __name__ == "__main__":
    batch_test_page()
```

### 8.2 Session Comparison

**Feature**: Compare two specific sessions side-by-side

```python
# eval_ui/pages/7_ðŸ”_Compare_Sessions.py
import streamlit as st
from eval_ui.database import fetch_session, fetch_traces, get_connection
from eval_ui.components.trace_viewer import display_trace

def compare_sessions_page():
    st.title("ðŸ” Compare Sessions")

    # Get all sessions
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("""
        SELECT id, topic, version_name, started_at
        FROM eval_sessions es
        JOIN agent_versions av ON es.agent_version_id = av.id
        WHERE es.status = 'completed'
        ORDER BY started_at DESC
        LIMIT 100
    """)
    sessions = cursor.fetchall()
    conn.close()

    session_options = {f"{s[0]} - {s[1][:50]} ({s[2]})": s[0] for s in sessions}

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("Session A")
        session_a = st.selectbox("Select first session",
                                 options=list(session_options.keys()),
                                 key='session_a')

    with col2:
        st.subheader("Session B")
        session_b = st.selectbox("Select second session",
                                 options=list(session_options.keys()),
                                 key='session_b')

    if session_a and session_b:
        session_a_id = session_options[session_a]
        session_b_id = session_options[session_b]

        col1, col2 = st.columns(2)

        with col1:
            display_trace(session_a_id)

        with col2:
            display_trace(session_b_id)
```

### 8.3 Export & Reporting

**File**: `eval_ui/pages/8_ðŸ“¤_Export.py`

```python
import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
from eval_ui.database import get_connection

def export_page():
    st.title("ðŸ“¤ Export & Reporting")

    # Date range filter
    col1, col2 = st.columns(2)
    with col1:
        start_date = st.date_input("From", value=datetime.now() - timedelta(days=30))
    with col2:
        end_date = st.date_input("To", value=datetime.now())

    # Version filter
    versions = fetch_all_versions()
    selected_versions = st.multiselect("Versions",
                                      [v['version_name'] for v in versions])

    if st.button("ðŸ“Š Generate Report"):
        conn = get_connection()

        # Build query
        query = """
            SELECT
                es.id as session_id,
                es.topic,
                es.started_at,
                es.completed_at,
                av.version_name,
                hr.rater_name,
                hr.quality_score,
                hr.coherence_score,
                hr.creativity_score,
                hr.overall_score,
                hr.comments,
                tm_agg.total_input_tokens,
                tm_agg.total_output_tokens,
                tm_agg.total_tokens,
                tm_agg.total_cost
            FROM eval_sessions es
            JOIN agent_versions av ON es.agent_version_id = av.id
            LEFT JOIN human_ratings hr ON es.id = hr.eval_session_id
            LEFT JOIN (
                SELECT
                    eval_session_id,
                    SUM(input_tokens) as total_input_tokens,
                    SUM(output_tokens) as total_output_tokens,
                    SUM(total_tokens) as total_tokens,
                    SUM(cost_usd) as total_cost
                FROM trace_metrics
                GROUP BY eval_session_id
            ) tm_agg ON es.id = tm_agg.eval_session_id
            WHERE DATE(es.started_at) BETWEEN ? AND ?
        """

        params = [start_date, end_date]

        if selected_versions:
            placeholders = ','.join('?' * len(selected_versions))
            query += f" AND av.version_name IN ({placeholders})"
            params.extend(selected_versions)

        query += " ORDER BY es.started_at DESC"

        df = pd.read_sql_query(query, conn, params=params)
        conn.close()

        # Display summary
        st.subheader("Summary Statistics")
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Total Sessions", len(df))
        col2.metric("Avg Quality", f"{df['overall_score'].mean():.2f}")
        col3.metric("Total Cost", f"${df['total_cost'].sum():.2f}")
        col4.metric("Total Tokens", f"{df['total_tokens'].sum():,.0f}")

        # Show data
        st.subheader("Detailed Data")
        st.dataframe(df)

        # Export options
        st.subheader("Export")

        col1, col2 = st.columns(2)

        with col1:
            csv = df.to_csv(index=False)
            st.download_button(
                label="ðŸ“¥ Download CSV",
                data=csv,
                file_name=f'evaluation_export_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
                mime='text/csv'
            )

        with col2:
            # Excel export (requires openpyxl)
            try:
                import io
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False, sheet_name='Evaluations')
                buffer.seek(0)

                st.download_button(
                    label="ðŸ“¥ Download Excel",
                    data=buffer,
                    file_name=f'evaluation_export_{datetime.now().strftime("%Y%m%d_%H%M%S")}.xlsx',
                    mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                )
            except ImportError:
                st.info("Install openpyxl to enable Excel export: pip install openpyxl")

if __name__ == "__main__":
    export_page()
```

---

## Risk Mitigation & Contingencies

### Risk 1: Token Extraction from Agno Fails

**Problem**: Agno might not expose token usage in response objects

**Mitigation Strategy**:
1. **Primary**: Extract from Anthropic API response objects (they always include usage)
2. **Secondary**: Query Phoenix OTEL traces via GraphQL API
3. **Tertiary**: Estimate based on character count (rough fallback)

**Implementation**:
```python
def get_token_usage(response, eval_session_id):
    # Try direct extraction
    usage = tracker.extract_usage_from_response(response)
    if usage[0] > 0:
        return usage

    # Fallback to Phoenix
    phoenix_traces = fetch_traces_from_phoenix(eval_session_id)
    if phoenix_traces:
        total_input = sum(t['input_tokens'] for t in phoenix_traces)
        total_output = sum(t['output_tokens'] for t in phoenix_traces)
        return (total_input, total_output)

    # Last resort: estimation (not recommended)
    return estimate_tokens(text)
```

### Risk 2: Database Concurrency Issues

**Problem**: Multiple processes (AgentOS, EvalUI, Phoenix) accessing SQLite

**Mitigation**:
1. Enable WAL mode: `PRAGMA journal_mode=WAL;`
2. Implement retry logic with exponential backoff
3. Use connection pooling for reads
4. Write operations are serialized by SQLite

**Implementation**:
```python
def get_connection_with_retry(max_retries=3):
    for i in range(max_retries):
        try:
            conn = sqlite3.connect('database.db', timeout=10)
            conn.execute("PRAGMA journal_mode=WAL")
            return conn
        except sqlite3.OperationalError as e:
            if i == max_retries - 1:
                raise
            time.sleep(0.1 * (2 ** i))  # Exponential backoff
```

### Risk 3: Version Config Drift

**Problem**: JSON snapshot might not capture all relevant changes

**Mitigation**:
1. Store Git commit hash with each version
2. JSON snapshot is metadata only
3. Actual code changes tracked in Git
4. Manual description field for human context

**Implementation**:
```python
def capture_current_config():
    import subprocess

    config = {
        'model_id': 'claude-sonnet-4-5-20250929',
        'git_commit': subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip(),
        'git_branch': subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD']).decode().strip(),
        # ... other config
    }
    return config
```

### Risk 4: Phoenix Not Running

**Problem**: Phoenix integration fails if service is down

**Mitigation**:
1. Make Phoenix integration optional
2. Check availability before querying
3. Show warning in UI if unreachable
4. Token tracking works independently

**Implementation**:
```python
def display_phoenix_status():
    if is_phoenix_available():
        st.success("âœ… Phoenix connected")
        st.markdown(f"[Open Phoenix Dashboard]({PHOENIX_URL})")
    else:
        st.warning("âš ï¸ Phoenix not available (optional)")
        st.info("Run: python -m phoenix.server.main serve")
```

### Risk 5: Slow Agent Execution

**Problem**: Writers waiting 30+ seconds for results

**Mitigation**:
1. Show real-time progress indicators in UI
2. Implement background job queue (optional)
3. Add caching for repeated topics
4. Batch mode for overnight runs

**Implementation**:
```python
with st.spinner('Generating story... This may take 15-30 seconds'):
    result = runner.run_with_tracking(topic)
```

---

## Validation Checklist

Before starting implementation, validate:

### Database
- [ ] SQLite version supports WAL mode (`sqlite3 --version`)
- [ ] Database file is writable
- [ ] No existing schema conflicts

### Python Environment
- [ ] Python 3.8+ installed
- [ ] All dependencies from requirements.txt installed
- [ ] `pip install streamlit plotly` completes successfully

### Network & Ports
- [ ] Port 8888 is available
- [ ] Phoenix accessible at localhost:6006 (if using)
- [ ] AgentOS accessible at localhost:8000

### Agent System
- [ ] Agno agents run successfully
- [ ] Can execute: `python agents.py` without errors
- [ ] Test data exists (at least 1 agent run completed)

### API Access
- [ ] Anthropic API key is valid
- [ ] API quota is sufficient for testing
- [ ] Response objects can be inspected

### Development Tools
- [ ] Git installed (for version tracking)
- [ ] Code editor configured
- [ ] Browser supports modern JavaScript (for Streamlit)

---

## Estimated Timeline

### Phase Breakdown

| Phase | Task | Time Estimate |
|-------|------|---------------|
| **Phase 1** | Database schema extension | 1 hour |
| | - Write SQL DDL statements | 30 min |
| | - Create setup_eval_ui.py script | 30 min |
| **Phase 2** | Token & cost tracking | 3 hours |
| | - Implement token_tracker.py | 1.5 hours |
| | - Test token extraction methods | 1 hour |
| | - Phoenix fallback integration | 30 min |
| **Phase 3** | UI application structure | 2 hours |
| | - Set up Streamlit app skeleton | 1 hour |
| | - Create file structure | 30 min |
| | - Basic navigation | 30 min |
| **Phase 4** | Core UI features | 6 hours |
| | - Trace viewer component | 1.5 hours |
| | - Rating form | 1.5 hours |
| | - Analytics dashboard | 2 hours |
| | - Database access layer | 1 hour |
| **Phase 5** | Version management | 2 hours |
| | - Versioning module | 1 hour |
| | - Version capture & config | 1 hour |
| **Phase 6** | Integration | 3 hours |
| | - Agent runner wrapper | 1.5 hours |
| | - Connect all components | 1 hour |
| | - Error handling | 30 min |
| **Phase 7** | Testing & polish | 3 hours |
| | - End-to-end testing | 1.5 hours |
| | - Bug fixes | 1 hour |
| | - UI improvements | 30 min |
| **Phase 8** | Additional features (optional) | 4 hours |
| | - Batch testing | 1.5 hours |
| | - Export functionality | 1 hour |
| | - Session comparison | 1.5 hours |

### Total Estimates

- **Core Features (Phases 1-7)**: ~20 hours (2.5 days of focused work)
- **With Additional Features**: ~24 hours (3 days)

### Realistic Schedule

- **Week 1**: Phases 1-3 (foundation)
- **Week 2**: Phases 4-6 (core features & integration)
- **Week 3**: Phase 7 (testing) + Phase 8 (enhancements)

---

## Success Criteria

The implementation is successful when:

### Functional Requirements
1. âœ… Human can generate story from UI and immediately see results
2. âœ… Token counts are accurate within 5% margin
3. âœ… Costs auto-calculate correctly based on token usage
4. âœ… Human can rate story on 1-10 scale with 4 dimensions
5. âœ… Ratings persist correctly in database
6. âœ… Versions are captured automatically on each run
7. âœ… Analytics show quality trends across versions
8. âœ… Cost comparison works across versions

### Performance Requirements
9. âœ… UI loads in < 2 seconds
10. âœ… Agent execution completes in < 60 seconds
11. âœ… Database queries return in < 1 second
12. âœ… System handles 50+ test sessions without degradation

### Usability Requirements
13. âœ… Writers can use the system without developer assistance
14. âœ… No technical errors visible to end users
15. âœ… Clear error messages when things go wrong
16. âœ… Intuitive navigation between pages

### Data Integrity Requirements
17. âœ… No data loss during concurrent operations
18. âœ… Version history is preserved correctly
19. âœ… Token/cost calculations are reproducible
20. âœ… Export functionality produces valid CSV/Excel files

### Integration Requirements
21. âœ… Works alongside existing Phoenix setup
22. âœ… Doesn't break existing AgentOS functionality
23. âœ… Database schema extends (not replaces) existing tables
24. âœ… Can run on different ports without conflicts

---

## Technology Decision: Streamlit vs React

### Comparison Matrix

| Criterion | Streamlit | React + FastAPI |
|-----------|-----------|-----------------|
| **Development Time** | 2-3 days | 1-2 weeks |
| **Python Skills Required** | âœ… Yes (native) | âš ï¸ Plus JavaScript |
| **Maintenance Complexity** | Low | High |
| **Customization Potential** | Medium | Very High |
| **Deployment** | Single command | Multi-step build |
| **Real-time Updates** | Built-in | Need WebSockets |
| **Form Handling** | Native | Manual implementation |
| **Charts/Graphs** | Plotly integration | Manual D3.js/Recharts |
| **Learning Curve** | Gentle | Steep |
| **For Internal Tools** | **Perfect** | Overkill |
| **For Production SaaS** | Limited | Better choice |

### Decision: **Use Streamlit**

**Rationale**:
- Internal tool for team of writers (not public-facing)
- Pure Python (matches your tech stack)
- Rapid development (2-3 days vs 2 weeks)
- Built-in components for all your needs
- Easy to maintain and modify
- No JavaScript knowledge required
- Streamlit Cloud deployment if needed later

**When to Reconsider**:
- If you need complex custom UI interactions
- If you plan to scale to 100+ concurrent users
- If you need mobile app integration
- If branding/design customization is critical

---

## Next Steps

### Immediate Actions

1. **Review this plan** with your team
2. **Confirm technology choice** (Streamlit recommended)
3. **Validate prerequisites** using the checklist
4. **Set up development environment**:
   ```bash
   pip install streamlit plotly openpyxl
   ```

### Implementation Order

**Start with Phase 1**:
```bash
# Create setup script
python setup_eval_ui.py

# Verify database schema
sqlite3 database.db ".schema"
```

**Then proceed sequentially** through phases 2-7.

### Communication

- Update this document as you discover new requirements
- Track blockers and solutions
- Document any deviations from the plan

---

## Appendix: File Checklist

Files to create during implementation:

### Core Files
- [ ] `setup_eval_ui.py` - Database setup script
- [ ] `token_tracker.py` - Token tracking utility
- [ ] `requirements-eval.txt` - Additional dependencies

### Eval UI Directory
- [ ] `eval_ui/__init__.py`
- [ ] `eval_ui/app.py` - Main Streamlit entry point
- [ ] `eval_ui/database.py` - DB access layer
- [ ] `eval_ui/versioning.py` - Version management
- [ ] `eval_ui/agent_runner.py` - Agent execution wrapper
- [ ] `eval_ui/phoenix_client.py` - Phoenix API integration

### UI Pages
- [ ] `eval_ui/pages/1_ðŸ _Dashboard.py`
- [ ] `eval_ui/pages/2_ðŸ§ª_Run_Test.py`
- [ ] `eval_ui/pages/3_ðŸ“Š_Review_Results.py`
- [ ] `eval_ui/pages/4_ðŸ“ˆ_Analytics.py`
- [ ] `eval_ui/pages/5_âš™ï¸_Settings.py`
- [ ] `eval_ui/pages/6_âš¡_Batch_Test.py` (optional)
- [ ] `eval_ui/pages/7_ðŸ”_Compare_Sessions.py` (optional)
- [ ] `eval_ui/pages/8_ðŸ“¤_Export.py` (optional)

### UI Components
- [ ] `eval_ui/components/rating_form.py`
- [ ] `eval_ui/components/trace_viewer.py`
- [ ] `eval_ui/components/charts.py`
- [ ] `eval_ui/components/session_card.py`

---

## Document Control

- **Version**: 1.0
- **Last Updated**: October 9, 2025
- **Author**: Claude
- **Status**: Ready for Implementation
- **Next Review**: After Phase 3 completion

---

*This plan is comprehensive and executable. All requirements are addressed with concrete implementations. Ready to proceed with Phase 1 when you give the signal.*
